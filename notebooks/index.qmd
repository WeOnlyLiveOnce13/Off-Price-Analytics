---
title: "FARO OFF-PRICE RETAIL ANALYTICS"
subtitle: "A data-driven approach to understanding what drives off-price sales"
title-block-banner-color: "#f8f9fa"
title-block-banner: true
author: "Dan A. Tshisungu"
date: today
description: "In this project, we attempt to provide insights on Faro and its retail business of discounted branded products."
toc: true
toc-title: "On this page"
toc-location: left
format: 
  html:
    page-layout: full
editor: 
  render-on-save: true
jupyter:
  kernelspec:
    name: faro
    language: python
    display_name: "Faro Kernel"
---

# I. INTRODUCTION

## I.1. Background

Faro, an off-price retailer that specialises in selling branded products at discounted prices. The business wants to understand which product types, brands, and\
suppliers move fastest, how pricing affects inventory turnover, and how customer shopping habits vary by region and time.

## I.2. Project Objective

The main objectives of this project are:

1.  clean and prepare the data for analysis by identifying possible quality issues.
2.  Perform a business-oriented exploratory data analysis (EDA) to identify key insights on how specific discounts and features influence the company's sales, business performance, and customer behavior.
3.  Build and evaluate a predictive model to predict whether a given product will be sold on clearance or not.

## I. 3. Data Overview:

We load the dataset and assess its structure and summary statistics.

**Dataset Description**

-   **TransactionID**: Unique ID for each transaction
-   **StoreID**: Store location identifier
-   **Date**: Date of transaction
-   **ProductID**: Unique product identifier
-   **Brand**: Product brand
-   **Supplier**: Supplier of the product
-   **Category**: Top-level product category (e.g., Apparel, Footwear, Accessories, Kids)
-   **Subcategory**: Second-level product classification (e.g., Tops, Boots, Bags, Boys)
-   **Type**: Third-level, most granular product type (e.g., T-shirt, Sneakers, Clutch, Puzzle)
-   **OriginalPrice**: Original list price
-   **DiscountedPrice**: Actual sale price
-   **Quantity**: Number of units sold (can be negative for returns)
-   **CustomerID**: Encrypted customer ID
-   **Region**: Store region (e.g., GP, WC, KZN)

# II. DATA PREPARATION

## II.1. Loading the dataset

```{python}
#| echo: false
#| include: false
import os

print("Current working directory:", os.getcwd())
```

We load the dataset and display the first 10 transactions.

```{python}
#| code-fold: true
#| label: data-loading

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import boxcox
from scipy.stats import skew

from sklearn.preprocessing import PowerTransformer, QuantileTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mutual_info_score
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report


from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from sklearn.tree import DecisionTreeClassifier


from lets_plot import *
from plotnine import *

LetsPlot.setup_html()

df = pd.read_csv('../data/raw/offprice_transactions.csv')
df.head(5)
```

## II.2. Dataset Inspection

We check the number of observations and features in the dataset.

```{python}
#| code-fold: true
#| label: data-shape
#| echo: false

print(f"Our dataset has {df.shape[0]} transactions and {df.shape[1]} features.")

```

::: callout-note
The dataset contains **100000 transactions (records)** and is composed of **15 features**.
:::

**Features types:**

We check the feature types to find out what feature may need conversion.

```{python}
#| code-fold: true
#| label: features-types

df.dtypes
```

::: {.callout-note collapse="true" title="Features Insights"}
-   `Date` feature has to be converted into a datetime feature, and temporal features (day, week, month) extracted from it.
-   Create `DiscountPercent` feature from the price features.
-   Create `Revenue` feature based on both `Price` and `Quantity`. Refer to @cau-revenue.
:::

**Feature Information:**

```{python}
#| code-fold: true
#| label: features-info

df.info()
```

::: {#cau-revenue .callout-caution title="Revenue formula" collapse="true"}
Some products are sold at normal price while others are sold at a discounted price.\
Thus, the formula for the `Revenue` feature is:

$$
\text{Revenue} =
\begin{cases}
\text{Quantity} \times \text{OriginalPrice}, & \text{if } \texttt{ClearanceFlag} = \texttt{No} \\
\text{Quantity} \times \text{DiscountedPrice}, & \text{if } \texttt{ClearanceFlag} = \texttt{Yes}
\end{cases}
$$
:::

## II.3. Data Quality issues

We now check any data quality issues:

-   duplicate values
-   missing values
-   skewness of the distribution
-   outliers
-   incorrect feature formating etc

### II.3.1. Duplicate values

```{python}
#| code-fold: true
#| label: duplicate-check

df.duplicated().any()
```

::: callout-note
There are no duplicate value in the dataset.
:::

### II.3.2. Missing value analysis

Below we check the number of missing value for each feature:

```{python}
#| code-fold: true
#| label: missing-value

df.isnull().any()
```

::: callout-note
There are no missing value in the dataset. There is therefore no need for handling incomplete records.
:::

### II.3.3. Distribution analysis

We check the distribution of both the numerical and categorical features. But first we extract numerical and categorical features.

```{python}
#| code-fold: true
#| label: feature-type-extraction

numerical_columns = df.select_dtypes(include="number").columns.to_list()

categorical_columns = df.select_dtypes(exclude="number").columns.to_list()

print(f" **Numerical features in the dataset:** {numerical_columns}")

print(f" **Categorical features in the dataset:** {categorical_columns}")
```

::: callout-caution
Remember we have to convert the `Date` feature later on.
:::

:::::: {.panel-tabset title="Distribution analysis"}
## Numerical features

```{python}
#|code-fold: true
#| label: numerical-stats
#| lst-label: lst-numerical-feature
#| lst-cap: Numerical features description 

df.describe()
```

We can also visualize the distribution below:

```{python}
#|code-fold: true   
#| label: numerical-distribution
numerical = ["OriginalPrice", "DiscountedPrice", "Quantity"]

for feat in numerical: 

    # Extract skewness
    print(f"{feat}:")
    skewness = skew(df[feat])

    # Print
    print(f"Skewness is {skewness}")
    
    # Generate plot
    p = (ggplot(df, aes(x=feat)) +
         geom_density(color='darkgreen', alpha=.7) +
         ggtitle(f"Distribution of {feat} (Skewness: {skewness:.2f})") +
         xlab(feat) + ylab("Density") +
         theme_minimal())

    # Show the plot
    display(p)

```

::: {.callout-note title="Numerical feature Insights" collapse="true"}
-   `OriginalPrice` feature is uniformly distributed meaning products are sold throughout the price range and no particular `OriginalPrice` is informative.
-   `DiscountedPrice` feature is skewed to the right meaning most products are sold at a lower `DiscountedPrice`, up to `R140`, compared to higher prices, above `R150`.
-   `Quantity` feature has a minimum of **-2** meaning one or two items returned every now and then.
:::

## Categorical features

```{python}
#|code-fold: true
#| label: categorical-count-distribution

df.describe(exclude="number")
```

We can have a more detailed view of the categorical features below:

```{python}
#|code-fold: true
#| label: categorical-distribution


categories = ["StoreID", "ProductID", "Brand", "Supplier", "Category", "Subcategory", "Type", "CustomerID", "Region"]

for col in categories:
    print(f"{col} unique values: {df[col].value_counts()}")
    print("--"*20)
```

::: {.callout-note title="Categorical features Insights" collapse="true"}
-   Some customers have multiple transactions, so `CustomerID` is not unique.
-   9000 unique products are sold
-   10 unique brands and stores
-   Products are bought in 7 different quantities, may need to investigate further for a correlation between quantity and price.
-   60470 unique customers, maybe investigate or cluster them based on their purchasing behavior.
:::

## Target feature : `ClearanceFlag`

```{python}
#|code-fold: true
#| label: target-variable


df["ClearanceFlag"].value_counts()
```

::: {.callout-note collapse="true" title="Target feature Insight"}
Out of 100000 items sold:

-   79980, \~80%, were not on clearance sale
-   20020, \~20%, were on clearance sale
-   Stores sell mostly items at the normal rate.
-   A highly imbalanced dataset (1:4 ratio) for building a predictive model.
:::
::::::

### II.3.4. Data preparation

We prepare the data for further analysis.

**A. Formatting features**

We convert the date feature in a datetime type, and we ensure numerical features are properly set.

```{python}
#|code-fold: true
#| label: feature-formatting


df['Date'] = pd.to_datetime(df['Date'])

df['OriginalPrice'] = pd.to_numeric(df['OriginalPrice'], errors='coerce')
df['DiscountedPrice'] = pd.to_numeric(df['DiscountedPrice'], errors='coerce')
df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')
```

**B. Feature preparation**

At this stage @lst-numerical-feature, we observed that the lowest of value of the `Quantity` feature is **-2**. We must take care of returned products as well create a `Revenue` @cau-revenue.

::: {.callout-tip title="Features"}
-   Every returned product was entered as a negative number. We create a new feature to describe if a product is return. e.g `IsReturn`.
-   Machine learning behave differently to the presence of negative numbers such as quantity = -1. We must consider both the business logic (A negative quantity means a return and thus an opportunity loss) and the model performance impact.
:::

```{python}
#|code-fold: true
#| label: return-feature


# 1. Separate quantities and create indicators
df['QuantityAbs'] = df['Quantity'].abs()
# Identify returns
df['IsReturn'] = (df['Quantity'] < 0).astype(int)
df['TransactionType'] = df['Quantity'].apply(lambda x: 'Return' if x < 0 else 'Sale')


# 2. Create revenue Revenue with proper handling
df['RevenueAbs'] = np.where(df['ClearanceFlag'] == 'No', 
                            df['QuantityAbs'] * df['OriginalPrice'],
                            df['QuantityAbs'] * df['DiscountedPrice'])


# 3. Create final Revenue (negative for returns)
df['RevenueFinal'] = np.where(df['IsReturn'] == 1, 
                              -df['RevenueAbs'], 
                              df['RevenueAbs'])

# 4. Drop original Quantity if desired
df = df.drop('Quantity', axis=1)


# 5. create time-based features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['WeekOfYear'] = df['Date'].dt.isocalendar().week
df['DayOfWeek'] = df['Date'].dt.dayofweek

# 6. Calculate discount percentage
df['DiscountPercentage'] = ((df['OriginalPrice'] - df['DiscountedPrice']) / df['OriginalPrice'] * 100).round(2)

df.head(5)
```

**C. Outlier Analysis**

We observed above that `DiscountedPrice` is skewed to the right indicating the presence of outliers.

There are many ways to detect outliers:

-   By calculating the skewness of the distribution and establishing a threshold
-   By visualizing the distribution of features
-   And more advanced methods more outlier detection
-   Using the IQR for the feature.

Above we applied the first two. We can confirm that with the last method:

For `DiscountedPrice`, we have:

-   min = 8
-   25% = 58
-   50% = 103
-   75% = 149.98
-   100% = 270

By finding the difference between each interval, we can deduce from where outliers occur:

-   25% - min = 50
-   50% - 25% = 45
-   75% - 50% = 46
-   100% - 75%= `120`

::: {.callout-note title="Outliers Insight"}
-   We notice a jump from 75th percentile to the 100th depicting outliers.
-   75th percentile + 50 = \~200. Anything above `200` in the `DiscountedPrice` is an outlier.
-   Though a mild skewness (0.35) is observed, we will attempt to deal with it.
:::

**Summary of transformations**

```{python}
#|code-fold: true
#| label: outlier-analysis

transformations = {}

# Original
transformations['Original'] = df['DiscountedPrice']

# Log transformation
transformations['Log'] = np.log1p(df['DiscountedPrice'])

# Square root
transformations['Sqrt'] = np.sqrt(df['DiscountedPrice'])

# Box-Cox
transformations['BoxCox'], _ = boxcox(df['DiscountedPrice'] + 1)

# Yeo-Johnson
pt = PowerTransformer(method='yeo-johnson', standardize=False)
transformations['YeoJohnson'] = pt.fit_transform(df[['DiscountedPrice']]).flatten()

# Compare skewness
results = []
for name, data in transformations.items():
    skew_val = stats.skew(data)
    results.append({'Transformation': name, 'Skewness': skew_val})

comparison_df = pd.DataFrame(results)
print(comparison_df.round(3))

# Find the best transformation (closest to 0)
best_transform = comparison_df.loc[comparison_df['Skewness'].abs().idxmin(), 'Transformation']
print(f"\nBest transformation: {best_transform}")
```

**Final implementation**

```{python}
#|code-fold: true
#| label: final-implementation

# Apply Box-Cox and store the Lambda value 
df['DiscountedPriceBoxCox'], lambda_val = boxcox(df['DiscountedPrice'] + 1)

# Drop original column
df = df.drop('DiscountedPrice', axis=1)


# Update your revenue calculation with transformed prices
df['RevenueAbs'] = np.where(df['ClearanceFlag'] == 'No', 
                           df['QuantityAbs'] * df['OriginalPrice'],
                           df['QuantityAbs'] * df['DiscountedPriceBoxCox'])

print(f"New skewness: {stats.skew(df['DiscountedPriceBoxCox']):.3f}")
df.head(5)
```

::: {.callout-note title="Summary of Data Preparation"}
-   We loaded our dataset and check its observations and features
-   We made sure there were no missing values nor any duplicated values
-   We created new features to enrich our dataset and make it more consistent for further analysis.
-   We converted `Date` and transformed the `DiscountedPrice` feature to handle the format for the former and outliers for the latter.
:::

# III. EXPLORATORY DATA ANALYSIS - EDA

We explore the dataset in depth and answer business questions.

Let us have a look at the updated dataset's features:

## III.1. Units and Revenue Analysis

### Sales information

```{python}
#|code-fold: true
#| label: data-overview

print(f"Sales transactions: {df[df['IsReturn'] == 0].shape[0]:,}")
print(f"Return transactions: {df[df['IsReturn'] == 1].shape[0]:,}")
print(f"Return rate: {df['IsReturn'].mean():.3f}")
```

::: {.callout-note title="Sales Insights"}
-   `Sales` account for  94.5% of all transactions while `returns` for ~ 5.1%.
:::

Because we focus on units sold and revenue, we filter to only use items that were not returned.

```{python}
#|code-fold: true
#| label: data-filter

df_sales = df[df['IsReturn'] == 0].copy()
print(f"\nAnalyzing {df_sales.shape[0]:,} sales transactions (excluding returns)")
```


### Units Analysis

:::{.panel-tabset title="Sales analysis"}

## Supplier by Units

```{python}
#| code-fold: true
#| label: supllier-units-plot

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

# Summarise
grouped_df = (
    df_sales.groupby(['Region', 'Supplier', 'ClearanceFlag'], as_index=False)
      .agg({'QuantityAbs': 'sum', 'RevenueFinal': 'sum'})
)

# Order Supplier by total quantity across all ClearanceFlags
supplier_order = (
    grouped_df.groupby('Supplier')['QuantityAbs'].sum()
    .sort_values().index.tolist()
)
grouped_df['Supplier'] = pd.Categorical(grouped_df['Supplier'], categories=supplier_order, ordered=True)

# Define colors
palette = {"Yes": "#faca80", "No": "#151931"}

# Prepare plot
fig, axes = plt.subplots(1, 3, figsize=(18, 9), sharey=True)
fig.subplots_adjust(top=0.85, bottom=0.15, left=0.05, right=0.95, wspace=0.25)

# Custom Title and Subtitle
highlight_color = "#faca80"
subtitle_color = "gray"
caption_color = "#666666"
text_color = "#151931"

fig.suptitle("Supplier Contribution to Quantity Sold", fontsize=18, fontweight='bold', y=0.98)
fig.text(0.5, 0.925,
         f"Comparison of Clearance status per Region",
         ha='center', fontsize=13, color=subtitle_color)

regions = grouped_df['Region'].unique()

for i, region in enumerate(regions):
    ax = axes[i]
    region_data = grouped_df[grouped_df['Region'] == region]

    suppliers = region_data['Supplier'].cat.categories
    clearance_yes = []
    clearance_no = []

    for supplier in suppliers:
        supplier_data = region_data[region_data['Supplier'] == supplier]
        yes_val = supplier_data[supplier_data['ClearanceFlag'] == 'Yes']['QuantityAbs'].sum()
        no_val = supplier_data[supplier_data['ClearanceFlag'] == 'No']['QuantityAbs'].sum()
        clearance_yes.append(yes_val)
        clearance_no.append(no_val)

    y_pos = range(len(suppliers))
    bar_height = 0.35

    bars_no = ax.barh([y - bar_height/2 for y in y_pos], clearance_no, 
                      bar_height, label='No', color=palette['No'])
    bars_yes = ax.barh([y + bar_height/2 for y in y_pos], clearance_yes, 
                       bar_height, label='Yes', color=palette['Yes'])

    # Add data labels
    for bar in bars_no:
        width = bar.get_width()
        if width > 0:
            ax.text(width + max(clearance_no) * 0.01, bar.get_y() + bar.get_height()/2,
                    f"{int(width):,}", va='center', ha='left',
                    fontsize=9, color=text_color, fontweight='bold')

    for bar in bars_yes:
        width = bar.get_width()
        if width > 0:
            ax.text(width + max(clearance_yes) * 0.01, bar.get_y() + bar.get_height()/2,
                    f"{int(width):,}", va='center', ha='left',
                    fontsize=9, color=text_color, fontweight='bold')

    ax.set_yticks(y_pos)
    ax.set_yticklabels(suppliers)
    ax.set_xlabel('Total Quantity', fontsize=11)
    ax.set_title(f"{region}", fontsize=13, fontweight='bold', color=text_color, pad=15)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f'{int(x):,}'))

    if i == 0:
        ax.legend(title='Clearance Sale', loc='upper right', fontsize=9)

# Add caption
fig.text(0.01, 0.02, 'Source: Dan - Faro', 
         ha='left', va='bottom', fontsize=9, color=caption_color)

plt.show()
```



## Brand by Units

```{python}
#| code-fold: true
#| label: brand-units-plot

# Summarise
grouped_df = (
    df_sales.groupby(['Region', 'Brand', 'ClearanceFlag'], as_index=False)
      .agg({'QuantityAbs': 'sum', 'RevenueFinal': 'sum'})
)

# Order Supplier by total quantity across all ClearanceFlags
brand_order = (
    grouped_df.groupby('Brand')['QuantityAbs'].sum()
    .sort_values().index.tolist()
)
grouped_df['Brand'] = pd.Categorical(grouped_df['Brand'], categories=brand_order, ordered=True)

# Define colors
palette = {"Yes": "#faca80", "No": "#151931"}

# Prepare plot
fig, axes = plt.subplots(1, 3, figsize=(18, 9), sharey=True)
fig.subplots_adjust(top=0.85, bottom=0.15, left=0.05, right=0.95, wspace=0.25)

# Custom Title and Subtitle
highlight_color = "#faca80"
subtitle_color = "gray"
caption_color = "#666666"
text_color = "#151931"

fig.suptitle("Brand Contribution to Quantity Sold", fontsize=18, fontweight='bold', y=0.98)
fig.text(0.5, 0.925,
         f"Comparison of Clearance status per Region",
         ha='center', fontsize=13, color=subtitle_color)

regions = grouped_df['Region'].unique()

for i, region in enumerate(regions):
    ax = axes[i]
    region_data = grouped_df[grouped_df['Region'] == region]

    brands = region_data['Brand'].cat.categories
    clearance_yes = []
    clearance_no = []

    for brand in brands:
        brand_data = region_data[region_data['Brand'] == brand]
        yes_val = brand_data[brand_data['ClearanceFlag'] == 'Yes']['QuantityAbs'].sum()
        no_val = brand_data[brand_data['ClearanceFlag'] == 'No']['QuantityAbs'].sum()
        clearance_yes.append(yes_val)
        clearance_no.append(no_val)

    y_pos = range(len(brands))
    bar_height = 0.35

    bars_no = ax.barh([y - bar_height/2 for y in y_pos], clearance_no, 
                      bar_height, label='No', color=palette['No'])
    bars_yes = ax.barh([y + bar_height/2 for y in y_pos], clearance_yes, 
                       bar_height, label='Yes', color=palette['Yes'])

    # Add data labels
    for bar in bars_no:
        width = bar.get_width()
        if width > 0:
            ax.text(width + max(clearance_no) * 0.01, bar.get_y() + bar.get_height()/2,
                    f"{int(width):,}", va='center', ha='left',
                    fontsize=9, color=text_color, fontweight='bold')

    for bar in bars_yes:
        width = bar.get_width()
        if width > 0:
            ax.text(width + max(clearance_yes) * 0.01, bar.get_y() + bar.get_height()/2,
                    f"{int(width):,}", va='center', ha='left',
                    fontsize=9, color=text_color, fontweight='bold')

    ax.set_yticks(y_pos)
    ax.set_yticklabels(brands)
    ax.set_xlabel('Total Quantity', fontsize=11)
    ax.set_title(f"{region}", fontsize=13, fontweight='bold', color=text_color, pad=15)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f'{int(x):,}'))

    if i == 0:
        ax.legend(title='Clearance Sale', loc='upper right', fontsize=9)

# Add caption
fig.text(0.01, 0.02, 'Source: Dan - Faro', 
         ha='left', va='bottom', fontsize=9, color=caption_color)

plt.show()

```

## Category by Units

```{python}
#| code-fold: true
#| label: category-units-plot

# Summarise
grouped_df = (
    df_sales.groupby(['Region', 'Category', 'ClearanceFlag'], as_index=False)
      .agg({'QuantityAbs': 'sum', 'RevenueFinal': 'sum'})
)


category_order = (
    grouped_df.groupby('Category')['QuantityAbs'].sum()
    .sort_values().index.tolist()
)
grouped_df['Category'] = pd.Categorical(grouped_df['Category'], categories=category_order, ordered=True)

# Define colors
palette = {"Yes": "#faca80", "No": "#151931"}

# Prepare plot
fig, axes = plt.subplots(1, 3, figsize=(18, 9), sharey=True)
fig.subplots_adjust(top=0.85, bottom=0.15, left=0.05, right=0.95, wspace=0.25)

# Custom Title and Subtitle
highlight_color = "#faca80"
subtitle_color = "gray"
caption_color = "#666666"
text_color = "#151931"

fig.suptitle("Category Contribution to Quantity Sold", fontsize=18, fontweight='bold', y=0.98)
fig.text(0.5, 0.925,
         f"Comparison of Clearance status per Region",
         ha='center', fontsize=13, color=subtitle_color)

regions = grouped_df['Region'].unique()

for i, region in enumerate(regions):
    ax = axes[i]
    region_data = grouped_df[grouped_df['Region'] == region]

    categories = region_data['Category'].cat.categories
    clearance_yes = []
    clearance_no = []

    for category in categories:
        category_data = region_data[region_data['Category'] == category]
        yes_val = category_data[category_data['ClearanceFlag'] == 'Yes']['QuantityAbs'].sum()
        no_val = category_data[category_data['ClearanceFlag'] == 'No']['QuantityAbs'].sum()
        clearance_yes.append(yes_val)
        clearance_no.append(no_val)

    y_pos = range(len(categories))
    bar_height = 0.35

    bars_no = ax.barh([y - bar_height/2 for y in y_pos], clearance_no, 
                      bar_height, label='No', color=palette['No'])
    bars_yes = ax.barh([y + bar_height/2 for y in y_pos], clearance_yes, 
                       bar_height, label='Yes', color=palette['Yes'])

    # Add data labels
    for bar in bars_no:
        width = bar.get_width()
        if width > 0:
            ax.text(width + max(clearance_no) * 0.01, bar.get_y() + bar.get_height()/2,
                    f"{int(width):,}", va='center', ha='left',
                    fontsize=9, color=text_color, fontweight='bold')

    for bar in bars_yes:
        width = bar.get_width()
        if width > 0:
            ax.text(width + max(clearance_yes) * 0.01, bar.get_y() + bar.get_height()/2,
                    f"{int(width):,}", va='center', ha='left',
                    fontsize=9, color=text_color, fontweight='bold')

    ax.set_yticks(y_pos)
    ax.set_yticklabels(categories)
    ax.set_xlabel('Total Quantity', fontsize=11)
    ax.set_title(f"{region}", fontsize=13, fontweight='bold', color=text_color, pad=15)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f'{int(x):,}'))

    if i == 0:
        ax.legend(title='Clearance Sale', loc='upper right', fontsize=9)

# Add caption
fig.text(0.01, 0.02, 'Source: Dan - Faro', 
         ha='left', va='bottom', fontsize=9, color=caption_color)

plt.show()
```


## Top Global Performers by Units

```{python}
#|code-fold: true
#| label: units-sold

units_by_category = df_sales.groupby('Category')['QuantityAbs'].sum().sort_values(ascending=False)
units_by_brand = df_sales.groupby('Brand')['QuantityAbs'].sum().sort_values(ascending=False).head(15)
units_by_supplier = df_sales.groupby('Supplier')['QuantityAbs'].sum().sort_values(ascending=False).head(15)


category_units_df = units_by_category.reset_index()
category_units_df.columns = ['Category', 'Units']

brand_units_df = units_by_brand.reset_index()
brand_units_df.columns = ['Brand', 'Units']

supplier_units_df = units_by_supplier.reset_index()
supplier_units_df.columns = ['Supplier', 'Units']

print("Top 5 Categories by units sold:")
for i, (cat, units) in enumerate(units_by_category.head().items(), 1):
    print(f"{i}. {cat}: {units:,} units")

print("\nTop 5 Brands by units sold:")
for i, (brand, units) in enumerate(units_by_brand.head().items(), 1):
    print(f"{i}. {brand}: {units:,} units")

print("\nTop 5 Suppliers by units sold:")
for i, (brand, units) in enumerate(units_by_supplier.head().items(), 1):
    print(f"{i}. {brand}: {units:,} units")

```

:::


### Revenue Analysis

:::{.panel-tabset title="Sales analysis"}

## Supplier by Revenue

```{python}
#|code-fold: true
#| label: supplier-revenue-plot

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter


grouped_df = (
    df_sales
      .groupby(['Region', 'Supplier', 'ClearanceFlag'], as_index=False)
      .agg({'QuantityAbs': 'sum',              
            'RevenueFinal': 'sum'})
)


supplier_order = (
    grouped_df.groupby('Supplier')['RevenueFinal']   
              .sum()
              .sort_values()
              .index
              .tolist()
)
grouped_df['Supplier'] = pd.Categorical(grouped_df['Supplier'],
                                        categories=supplier_order,
                                        ordered=True)


palette        = {"Yes": "#faca80", "No": "#151931"}
highlight_col  = "#faca80"
subtitle_col   = "gray"
caption_col    = "#666666"
text_col       = "#151931"


fig, axes = plt.subplots(1, 3, figsize=(18, 9), sharey=True)
fig.subplots_adjust(top=0.85, bottom=0.15, left=0.05,
                    right=0.95, wspace=0.25)

fig.suptitle("Supplier Contribution to Revenue",
             fontsize=18, fontweight='bold', y=0.98)

fig.text(0.5, 0.925,
         "Clearance vs Regular sales by region",
         ha='center', fontsize=13, color=subtitle_col)

regions = grouped_df['Region'].unique()


for i, region in enumerate(regions):
    ax = axes[i]
    region_data = grouped_df[grouped_df['Region'] == region]

    suppliers       = region_data['Supplier'].cat.categories
    clearance_yes   = []
    clearance_no    = []

    
    for supplier in suppliers:
        data = region_data[region_data['Supplier'] == supplier]
        yes_val = data[data['ClearanceFlag'] == 'Yes']['RevenueFinal'].sum()
        no_val  = data[data['ClearanceFlag'] == 'No']['RevenueFinal'].sum()
        clearance_yes.append(yes_val)
        clearance_no.append(no_val)

    y_pos      = range(len(suppliers))
    bar_height = 0.35

    bars_no = ax.barh([y - bar_height/2 for y in y_pos],
                      clearance_no, bar_height,
                      label='No', color=palette['No'])
    bars_yes = ax.barh([y + bar_height/2 for y in y_pos],
                       clearance_yes, bar_height,
                       label='Yes', color=palette['Yes'])

    
    def _add_labels(bars, offset):
        for bar in bars:
            width = bar.get_width()
            if width > 0:
                ax.text(width + offset, bar.get_y() + bar.get_height()/2,
                        f"{width:,.0f}",      
                        va='center', ha='left',
                        fontsize=9, color=text_col, fontweight='bold')
                

    _add_labels(bars_no, max(clearance_no) * 0.01 if clearance_no else 0)
    _add_labels(bars_yes, max(clearance_yes) * 0.01 if clearance_yes else 0)

    
    ax.set_yticks(y_pos)
    ax.set_yticklabels(suppliers)
    ax.set_xlabel('Total Revenue', fontsize=11)
    ax.set_title(region, fontsize=13, fontweight='bold',
                 color=text_col, pad=15)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    
    ax.xaxis.set_major_formatter(FuncFormatter(
        lambda x, _: f"R {x:,.0f}"      
        
    ))
    ax.tick_params(axis='x', labelrotation=30)

    if i == 0:
        ax.legend(title='Clearance Sale', loc='upper right', fontsize=9)


fig.text(0.01, 0.02, 'Source: Dan – Faro',
         ha='left', va='bottom', fontsize=9, color=caption_col)

plt.show()
```

## Brand by Revenue

```{python}
#|code-fold: true
#| label: brand-revenue-plot

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter


grouped_df = (
    df_sales
      .groupby(['Region', 'Brand', 'ClearanceFlag'], as_index=False)
      .agg({'QuantityAbs': 'sum',              
            'RevenueFinal': 'sum'})
)


brand_order = (
    grouped_df.groupby('Brand')['RevenueFinal']   
              .sum()
              .sort_values()
              .index
              .tolist()
)
grouped_df['Brand'] = pd.Categorical(grouped_df['Brand'],
                                        categories=brand_order,
                                        ordered=True)


palette        = {"Yes": "#faca80", "No": "#151931"}
highlight_col  = "#faca80"
subtitle_col   = "gray"
caption_col    = "#666666"
text_col       = "#151931"


fig, axes = plt.subplots(1, 3, figsize=(18, 9), sharey=True)
fig.subplots_adjust(top=0.85, bottom=0.15, left=0.05,
                    right=0.95, wspace=0.25)

fig.suptitle("Brand Contribution to Revenue",
             fontsize=18, fontweight='bold', y=0.98)

fig.text(0.5, 0.925,
         "Clearance status revenue by region",
         ha='center', fontsize=13, color=subtitle_col)

regions = grouped_df['Region'].unique()


for i, region in enumerate(regions):
    ax = axes[i]
    region_data = grouped_df[grouped_df['Region'] == region]

    brands       = region_data['Brand'].cat.categories
    clearance_yes   = []
    clearance_no    = []

    
    for brand in brands:
        data = region_data[region_data['Brand'] == brand]
        yes_val = data[data['ClearanceFlag'] == 'Yes']['RevenueFinal'].sum()
        no_val  = data[data['ClearanceFlag'] == 'No']['RevenueFinal'].sum()
        clearance_yes.append(yes_val)
        clearance_no.append(no_val)

    y_pos      = range(len(brands))
    bar_height = 0.35

    bars_no = ax.barh([y - bar_height/2 for y in y_pos],
                      clearance_no, bar_height,
                      label='No', color=palette['No'])
    bars_yes = ax.barh([y + bar_height/2 for y in y_pos],
                       clearance_yes, bar_height,
                       label='Yes', color=palette['Yes'])

    
    def _add_labels(bars, offset):
        for bar in bars:
            width = bar.get_width()
            if width > 0:
                ax.text(width + offset, bar.get_y() + bar.get_height()/2,
                        f"{width:,.0f}",      
                        va='center', ha='left',
                        fontsize=9, color=text_col, fontweight='bold'
                        )

    _add_labels(bars_no, max(clearance_no) * 0.01 if clearance_no else 0)
    _add_labels(bars_yes, max(clearance_yes) * 0.01 if clearance_yes else 0)

    
    ax.set_yticks(y_pos)
    ax.set_yticklabels(brands)
    ax.set_xlabel('Total Revenue', fontsize=11)
    ax.set_title(region, fontsize=13, fontweight='bold',
                 color=text_col, pad=15)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    
    ax.xaxis.set_major_formatter(FuncFormatter(
        lambda x, _: f"R {x:,.0f}"      
        
    ))

    ax.tick_params(axis='x', labelrotation=30)

    if i == 0:
        ax.legend(title='Clearance Sale', loc='upper right', fontsize=9)


fig.text(0.01, 0.02, 'Source: Dan – Faro',
         ha='left', va='bottom', fontsize=9, color=caption_col)

plt.show()
```

## Category by Revenue

```{python}
#|code-fold: true
#| label: category-revenue-plot

grouped_df = (
    df_sales
      .groupby(['Region', 'Category', 'ClearanceFlag'], as_index=False)
      .agg({'QuantityAbs': 'sum',              
            'RevenueFinal': 'sum'})
)


category_order = (
    grouped_df.groupby('Category')['RevenueFinal']   
              .sum()
              .sort_values()
              .index
              .tolist()
)
grouped_df['Category'] = pd.Categorical(grouped_df['Category'],
                                        categories=category_order,
                                        ordered=True)


palette        = {"Yes": "#faca80", "No": "#151931"}
highlight_col  = "#faca80"
subtitle_col   = "gray"
caption_col    = "#666666"
text_col       = "#151931"


fig, axes = plt.subplots(1, 3, figsize=(18, 9), sharey=True)
fig.subplots_adjust(top=0.85, bottom=0.15, left=0.05,
                    right=0.95, wspace=0.25)

fig.suptitle("Category Contribution to Revenue",
             fontsize=18, fontweight='bold', y=0.98)

fig.text(0.5, 0.925,
         "Clearance status revenue by region",
         ha='center', fontsize=13, color=subtitle_col)

regions = grouped_df['Region'].unique()


for i, region in enumerate(regions):
    ax = axes[i]
    region_data = grouped_df[grouped_df['Region'] == region]

    categories       = region_data['Category'].cat.categories
    clearance_yes   = []
    clearance_no    = []

    
    for category in categories:
        data = region_data[region_data['Category'] == category]
        yes_val = data[data['ClearanceFlag'] == 'Yes']['RevenueFinal'].sum()
        no_val  = data[data['ClearanceFlag'] == 'No']['RevenueFinal'].sum()
        clearance_yes.append(yes_val)
        clearance_no.append(no_val)

    y_pos      = range(len(categories))
    bar_height = 0.35

    bars_no = ax.barh([y - bar_height/2 for y in y_pos],
                      clearance_no, bar_height,
                      label='No', color=palette['No'])
    bars_yes = ax.barh([y + bar_height/2 for y in y_pos],
                       clearance_yes, bar_height,
                       label='Yes', color=palette['Yes'])

    
    def _add_labels(bars, offset):
        for bar in bars:
            width = bar.get_width()
            if width > 0:
                ax.text(width + offset, bar.get_y() + bar.get_height()/2,
                        f"{width:,.0f}",      
                        va='center', ha='left',
                        fontsize=9, color=text_col, fontweight='bold'
                        )

    _add_labels(bars_no, max(clearance_no) * 0.01 if clearance_no else 0)
    _add_labels(bars_yes, max(clearance_yes) * 0.01 if clearance_yes else 0)

    
    ax.set_yticks(y_pos)
    ax.set_yticklabels(categories)
    ax.set_xlabel('Total Revenue', fontsize=11)
    ax.set_title(region, fontsize=13, fontweight='bold',
                 color=text_col, pad=15)
    ax.grid(axis='x', alpha=0.3, linestyle='--')

    
    ax.xaxis.set_major_formatter(FuncFormatter(
        lambda x, _: f"R {x:,.0f}"      
        
    ))

    ax.tick_params(axis='x', labelrotation=30)

    if i == 0:
        ax.legend(title='Clearance Sale', loc='upper right', fontsize=9)


fig.text(0.01, 0.02, 'Source: Dan – Faro',
         ha='left', va='bottom', fontsize=9, color=caption_col)

plt.show()
```


## Top Global Performers by Revenue

```{python}
#|code-fold: true
#| label: revenue-made

revenue_by_category = df_sales.groupby('Category')['RevenueFinal'].sum().sort_values(ascending=False)
revenue_by_brand = df_sales.groupby('Brand')['RevenueFinal'].sum().sort_values(ascending=False).head(15)
revenue_by_supplier = df_sales.groupby('Supplier')['RevenueFinal'].sum().sort_values(ascending=False).head(15)


category_revenue_df = revenue_by_category.reset_index()
category_revenue_df.columns = ['Category', 'Revenue']


brand_revenue_df = revenue_by_brand.reset_index()
brand_revenue_df.columns = ['Brand', 'Revenue']

supplier_revenue_df = revenue_by_supplier.reset_index()
supplier_revenue_df.columns = ['Supplier', 'Revenue']



print("\nTOP 5 Categories by Revenue:")
for i, (cat, rev) in enumerate(revenue_by_category.head().items(), 1):
    print(f"{i}. {cat}: R {rev:,.2f}")


print("\nTOP 5 Brands by Revenue:")
for i, (brand, rev) in enumerate(revenue_by_brand.head().items(), 1):
    print(f"{i}. {brand}: R {rev:,.2f}")


print("\nTOP 5 Suppliers by Revenue:")
for i, (brand, rev) in enumerate(units_by_supplier.head().items(), 1):
    print(f"{i}. {brand}: R {rev:,.2f}")
```

:::


:::{.callout-note title="Insights of Analysis"}
-   In all three regions (GP, KZN, WC),`Asos` is the top performer *supplier* selling the most units both on sales and at normal price. Thus generating also the most revenue.
-   Based on the number of units sold:
    -   In GP, `Tommy Hilfiger` is the top *brand* selling the most units at normal prices and the second best on sales products.
    -   In KZN, `Ralph Lauren` is the top *brand* selling the most units both on sales and at normal prices.
    -   In WC, `Adidas` is the top *brand* selling the most units at normal price and `Ralph Lauren` is the top *brand* selling the most units on sales. 
    -   `Tommy Hilfiger` is the top performer *brand* in **units** sold. Followed by `Ralph Lauren` and `Under Armour`. 
    -   `Tommy Hilfiger` is the top performer *brand* in **revenue**. Followed by `DKNY` and `Under Armour`.
-   `Accessories` is the top performer `Category` followed by `Apparel` and `Kids` in **units** sold while `Kids`, `Apparel`, and `Accessories` are the top performers in **revenue**.
:::


## III.2. Discount effect on top brands & suppliers

**Supplier Discount % and units sold**

```{python}
#| code-fold: true
#| label: supplier-discount-units-plot

# discount data for scatter plot
discount_scatter_df = df_sales[df_sales['DiscountPercentage'] > 0].sample(min(5000, len(df_sales[df_sales['DiscountPercentage'] > 0]))).copy()

# discount distribution
discount_dist_df = df_sales[df_sales['DiscountPercentage'] > 0].copy()

# Group by Supplier
supplier_summary = (
    discount_dist_df.groupby('Supplier')
    .agg(TotalUnitsSold=('QuantityAbs', 'sum'), AvgDiscountPercent=('DiscountPercentage', 'mean'))
    .reset_index()
)

# Group by Brand
brand_summary = (
    discount_dist_df.groupby('Brand')
    .agg(TotalUnitsSold=('QuantityAbs', 'sum'), AvgDiscountPercent=('DiscountPercentage', 'mean'))
    .reset_index()
)

# Select top 10 suppliers
top_suppliers = supplier_summary.nlargest(10, 'TotalUnitsSold')

# Select top 10 brands
top_brands = brand_summary.nlargest(10, 'TotalUnitsSold')

p_discount_scatter = (
    ggplot(top_suppliers, aes(x='AvgDiscountPercent', y='TotalUnitsSold', label='Supplier')) +
    geom_point(color='#faca80', size=4, alpha=0.8) +
    geom_text(nudge_y=5, size=8) +
    labs(
        title='Top 7 Suppliers: Discount % vs. Units Sold',
        x='Average Discount Percent',
        y='Total Units Sold'
    ) +
    theme_minimal() +
    theme(figure_size=(6, 5))
)
p_discount_scatter
```

::: {.callout-note title="Supplier discount % and Units"}
-   `Asos`, top performer supplier in units sold, has an average discount percentage of **32.3%**, whereas `Nordstrom` and `Bloomingda` have an average discount percentage of **43.3%** with a performance of more or less 42,000 units, same as `Macy's` with an average discount percentage of **25%**. Depicting how the discount percentage does not drive sales.
:::


## III.3. Average turnover per store

**Turnover Statistics**

```{python}
#| code-fold: true
#| label: avg-sales

# Calculate weekly sales per store (sales only)
weekly_sales = df_sales.groupby(['StoreID', 'WeekOfYear'])['QuantityAbs'].sum().reset_index()
avg_weekly_turnover = weekly_sales.groupby('StoreID')['QuantityAbs'].mean().sort_values(ascending=False)


print("Stores performance KPI:")
print(f"-   Best performing store: {avg_weekly_turnover.index[0]} ({avg_weekly_turnover.iloc[0]:.1f} units/week)")
print(f"-   Average weekly turnover across all stores: {avg_weekly_turnover.mean():.1f} units/week")
print(f"-   Median weekly turnover: {avg_weekly_turnover.median():.1f} units/week")
print(f"-   Standard deviation: {avg_weekly_turnover.std():.1f} units/week")

print("\nTop 5 Stores by weekly turnover:")
for i, (store, turnover) in enumerate(avg_weekly_turnover.head(5).items(), 1):
    print(f"{i}. Store {store}: {turnover:.1f} units/week")
```

**Visualizations**


```{python}
#| code-fold: true
#| label: plot-top-stores

turnover_df = avg_weekly_turnover.reset_index()
turnover_df.columns = ['StoreID', 'AvgWeeklyTurnover']

stores_df = turnover_df.head(10)


# Stores Performance
p_avg_turnover = (ggplot(stores_df, 
        aes(x='reorder(StoreID, AvgWeeklyTurnover)', y='AvgWeeklyTurnover')
    ) +
      geom_col(fill='#151931', alpha=0.8) +
      coord_flip() +
      labs(title='Average Weekly Turnover by Store', 
           x='Store ID', 
           y='Average Weekly Units Sold') +
      theme_minimal() +
      theme(figure_size=(6, 5)))

p_avg_turnover
```

**Weekly trend**

```{python}
#| code-fold: true
#| label: store-week-trend

weekly_trend = df_sales.groupby('WeekOfYear')['QuantityAbs'].sum().reset_index()
weekly_trend['WeekOfYear'] = weekly_trend['WeekOfYear'].astype(int)


p_trend = (ggplot(weekly_trend, aes(x='WeekOfYear', y='QuantityAbs')) +
       geom_line(color='#faca80', size=1) +
       geom_point(color='#151931', size=2) +
       labs(title='Weekly Sales Trend Across All Stores', 
            x='Week of Year', 
            y='Total Units Sold') +
       theme_minimal() +
       theme(figure_size=(10, 5)))

p_trend
```

::: {.callout-note collapse="true" title="Stores Insights"}
-   `Stores` sell on average 1424 units per week and only varies by **9** units indicating they sell more or less the same quantity.
-   `S002` is the best performing store with 1438 units per week compared to the average of 1424 units per week.
:::

## III.4. Clearance Impact

**Sales only**


```{python}
#| code-fold: true
#| label: sales-plot-by-clearance


import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import FuncFormatter

PALETTE = {"No": "#faca80", "Yes": "#151931"}
currency = FuncFormatter(lambda x, _: f"${x:,.0f}")

def quartile_table(df, group_col, value_col):
    q = (df.groupby(group_col)[value_col]
            .quantile([.25, .5, .75]).unstack())
    q.columns = ['Q1', 'Median', 'Q3']
    return q.reset_index()

def annotate_quartiles(ax, stats, x_offset=0.06):
    for xpos, (_, row) in enumerate(stats.iterrows()):
        ax.scatter(xpos, row.Median, s=80,
                   facecolor='white', edgecolor='#151931',
                   linewidth=1.4, zorder=3)
        ax.text(xpos + x_offset, row.Median,
                f"Median: R {row.Median:,.0f}", va='center',
                ha='left', fontsize=9, fontweight='bold',
                color='#151931')
        ax.text(xpos - x_offset, row.Q1,
                f"Q1: R {row.Q1:,.0f}", va='center', ha='right',
                fontsize=8, color='#151931')
        ax.text(xpos - x_offset, row.Q3,
                f"Q3: R {row.Q3:,.0f}", va='center', ha='right',
                fontsize=8, color='#151931')

def clearance_plot(df, title):
    """Half-eye (left-half violin) + boxplot + annotated quartiles."""
    df = df.copy()
    df["_half"] = True  # dummy hue level for split violin

    fig, ax = plt.subplots(figsize=(6, 6))

    # Use dummy palette for _half
    dummy_palette = {True: "#b0b0b8"}

    # half violin (left side)
    sns.violinplot(
        data=df, x='ClearanceFlag', y='RevenueFinal',
        hue='_half',
        split=True,
        bw_adjust=.6,
        cut=0,
        palette=dummy_palette,  
        legend=False,
        inner=None, linewidth=0,
        ax=ax
    )

    # 2oxplot on top, with correct PALETTE
    sns.boxplot(
        data=df, x='ClearanceFlag', y='RevenueFinal',
        hue='ClearanceFlag', palette=PALETTE,
        legend=False, dodge=False,
        width=.18, fliersize=0,
        boxprops={'alpha': .9}, ax=ax
    )

    # annotate quartiles
    stats = quartile_table(df, 'ClearanceFlag', 'RevenueFinal')
    annotate_quartiles(ax, stats)

    # Final formatting
    ax.set_xlabel(None)
    ax.set_ylabel('Revenue (per line)', fontsize=11)
    ax.set_xticks([0, 1])
    ax.set_xticklabels(['Regular Price', 'Clearance'],
                       fontsize=11, fontweight='bold', color='#151931')
    ax.set_title(title, fontsize=14, weight='bold')
    ax.yaxis.set_major_formatter(currency)
    sns.despine(left=True)
    fig.tight_layout()
    return fig

# Build plots

sales_df = df[df['IsReturn'] == 0].copy()
returns_df = df[df['IsReturn'] == 1].copy()   

fig_sales = clearance_plot(
    sales_df,
    "Sales Revenue Distribution by Clearance Status"
)

if not returns_df.empty:
    fig_returns = clearance_plot(
        returns_df,
        "Return Revenue Distribution by Clearance Status"
    )

plt.show()
```



```{python}
#| code-fold: true
#| label: clearance-impact

# Sales only
clearance_sales_analysis = df_sales.groupby('ClearanceFlag').agg({
    'QuantityAbs': ['sum', 'mean'],
    'RevenueFinal': ['sum', 'mean'],
}).round(2)




print("Clearance vs Non-Clearance (Sales Only):")
print(clearance_sales_analysis)

```

**Returns and Sales**

```{python}
#| code-fold: true
#| label: clearance-return-sales

# Sales and returns transactions
clearance_return_analysis = df.groupby('ClearanceFlag').agg({
    'IsReturn': ['sum', 'mean'],
    'RevenueFinal': ['sum', 'mean'],
    'QuantityAbs': 'sum'
}).round(3)

print("Sales and returns")
print(clearance_return_analysis)

print("\nRETURN ANALYSIS (All Transactions):")
print("Non-Clearance:")
print(f"  - Return rate: {clearance_return_analysis.loc['No', ('IsReturn', 'mean')]:.3f}")
print(f"  - Total return transactions: {clearance_return_analysis.loc['No', ('IsReturn', 'sum')]:,.0f}")

print("Clearance:")
print(f"  - Return rate: {clearance_return_analysis.loc['Yes', ('IsReturn', 'mean')]:.3f}")
print(f"  - Total return transactions: {clearance_return_analysis.loc['Yes', ('IsReturn', 'sum')]:,.0f}")
```



::: {.callout-note collapse="true" title="Clearance Impact Insights"}
-   Faro has sold more items at normal price (227842 units for R36,373,670.00) than at discount price ( 56837 units for R4,332,340.98).
-   The return rates for items by their clearance status is roughly the same, 5.1% when sold at not price and 4.9% when sold at discounted price indicating that clearance does not impact the return.
:::

## III.5. Customer behavior

```{python}
#| code-fold: true
#| label: customer-behavior


regional_sales_analysis = df_sales.groupby('Region').agg({
    'CustomerID': 'nunique',
    'QuantityAbs': ['sum', 'mean'],
    'RevenueFinal': ['sum', 'mean'],
    'DiscountPercentage': 'mean',
    'TransactionID': 'count'
}).round(2)

regional_return_analysis = df.groupby('Region')['IsReturn'].mean()

print("Regional Sales KPI:")
print(regional_sales_analysis)

print("\nRegional Returns KPI:")
for region, rate in regional_return_analysis.items():
    print(f"{region}: {rate:.3f}")
```

**Regional metrics**

```{python}
#| code-fold: true
#| label: customer-behavior-summary

regional_summary = df_sales.groupby('Region').agg({
    'CustomerID': 'nunique',
    'TransactionID': 'count',
    'RevenueFinal': 'sum',
    'QuantityAbs': 'sum'
}).reset_index()

regional_summary['TransactionsPerCustomer'] = regional_summary['TransactionID'] / regional_summary['CustomerID']
regional_summary['RevenuePerCustomer'] = regional_summary['RevenueFinal'] / regional_summary['CustomerID']
regional_summary['UnitsPerCustomer'] = regional_summary['QuantityAbs'] / regional_summary['CustomerID']

# Add return rates
regional_summary = regional_summary.merge(
    regional_return_analysis.reset_index().rename(columns={'IsReturn': 'ReturnRate'}),
    on='Region'
)

print("\nCustomer-based Regional KPI:")
print(regional_summary[['Region', 'CustomerID', 'TransactionsPerCustomer', 'RevenuePerCustomer', 'UnitsPerCustomer', 'ReturnRate']])
```

::: {.callout-note collapse="true" title="Customer behavior Insights"}
-   `Regions` have roughly the same number of unique customers and generating the same revenue.
-   `Regions` have the same number of returns items indicating that no particular region is underperforming.
:::


# IV. PREDICTIVE MODELLING

**State of the dataset**

```{python}
#| code-fold: true
#| label: state-of-the-dataset

print("\nData types:")
df.info()
```

:::{.callout-note}

-   `TransactionID` is not informative for the model because of high cardinality.
-   `ProductID` is not informative for the model because of high cardinality.
:::

```{python}
#| code-fold: true
#| label: drop-high-cardinality

df = df.drop(['TransactionID', 'ProductID'], axis=1)
```

## IV.1. Target Feature

`ClearanceFlag` is the target feature of interest. We attempt to predict whether a product will be sold at clearance or at normal price. 
This will guide the marketing team to know when, how, and for which products to promote clearance sales to maximize revenue. It is, however, a tricky task as **any** product WILL definitely be sold at clearance.

Thus the goal is to penalize prediction of `Yes` when the product would have been sold at normal price.

```{python}
#| code-fold: true
#| label: ClearanceFlag-counts

df["ClearanceFlag"] = (df["ClearanceFlag"] == 'Yes').astype(int)

df["ClearanceFlag"].value_counts()
```

::: {.callout-note}
-   A **1:4** ratio between `Yes` and `No` for clearance status displaying imbalance.
:::


## IV.2. Feature Selection

Feature selection is a very important step in machine learning, just like the saying `garbage in, garbage out`, feeding the wrong features may negatively impact the model performance.

There are numerous way to select what features, recursive selection, mutual information, to more advanced methods making use of evolutionary algorithms such as particle swarm optimization or grey wolf.

In this project, we will follow a simple `mutual information(MI)`, a concept from information theory telling us how much we can learn from a feature if we know the value of of another.

**Goal:** I first want to establish a baseline model without performing any advanced selection or engineering of features.


### Numerical and Categorical features

:::{.panel-tabset title="Feature selection"}

## Correlation for Numerical 

```{python}
#| code-fold: true
#| label: numerical-corr
numerical_cols = ['OriginalPrice', 'QuantityAbs','RevenueAbs', 'RevenueFinal', 'Month', 'WeekOfYear', 'DayOfWeek', 'IsReturn','DiscountPercentage','DiscountedPriceBoxCox']


df[numerical_cols].corrwith(df.ClearanceFlag).to_frame('correlation')
```

:::{.callout-note}
-   `DiscountPercentage` has a *0.74* correlation with `ClearanceFlag` indicating that higher discount percentage likely to sell the product at clearance which is obvious. Maybe binning to find out what range of percentage is likely to sell at clearance.
-   `RevenueAbs` (-0.47) and `RevenueFinal` (-0.25) indicate that when revenue increases, the item is unlikely to have been sold at clearance.
-   A high `DiscountedPrice` will reduce the likelihood of the item being sold at clearance. Maybe binning the feature for better interpretability and possibly improve model performance.
:::

## Mutual Information for Categorical

```{python}

#| code-fold: true
#| label: categorical-mi

categorical_cols = ['StoreID', 'Brand', 'Supplier', 'Category', 'Subcategory', 'Type', 'Region','TransactionType']



### ----- Define the function to calculate the M.I on the training set ONLY
def calculate_mi(series):
    return mutual_info_score(series, df["ClearanceFlag"])

#### ---- Calculate MI between 'y' and the categorical variables of the training set ONLY 
df_mi = df[categorical_cols].apply(calculate_mi)
df_mi = df_mi.sort_values(ascending=False).to_frame(name='MI')

print('Below are the variable with highest M.I score:')
display(df_mi.head(15))
```


:::{.callout-note}
-   Knowing the `Type` of the product provides more information on whether or not it will be sold at clearance.
-   `TransactionType` values are not very informative.
-   `Brand`, `StoreID`, `Region`, and `Subcategory` values also provide valuable information on the target feature status. Maybe creating interaction between these categorical features might be helpful for a better generalization.
:::

:::

### Base model

#### 1. Select features

```{python}
#| code-fold: true
#| label: feature-selection

# Based on correlation
selected_num = ['OriginalPrice', 'QuantityAbs','RevenueAbs', 'RevenueFinal', 'DiscountPercentage','DiscountedPriceBoxCox']

# Based on MI
selected_cat = ['StoreID', 'Brand', 'Supplier', 'Category', 'Subcategory', 'Type', 'Region']
```

#### 2. Split

```{python}


# Split
df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=42)
df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=42)



## ---- Extract and create "ClearanceFlag" for the different splits
y_train = df_train["ClearanceFlag"].values
y_val = df_val["ClearanceFlag"].values
y_test = df_test["ClearanceFlag"].values

### ------ Delete "ClearanceFlag"from the splits
del df_train['ClearanceFlag']
del df_val['ClearanceFlag']
del df_test['ClearanceFlag']

len(df_train),len(df_val),len(df_test)
```


#### 3. Train base model


::::::{.panel-tabset title="Base model"}

# Base model with "Revenue"
```{python}
#| code-fold: true
#| label: base-model

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

## ----- Initialize the encoder: ------
dv = DictVectorizer(sparse=False)

## ---- Apply the transformation on the training set

train_dict = df_train[selected_num + selected_cat].to_dict(orient='records')
X_train = dv.fit_transform(train_dict)

## ---- Apply the transformation on the validation set (for evaluation)
val_dict = df_val[selected_num + selected_cat].to_dict(orient='records')
X_val = dv.transform(val_dict)

#### Training on Logistic regression

model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state = 42)
model.fit(X_train, y_train)


# Predict on validation set
y_pred = model.predict(X_val)

# Compute metrics
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')  
recall = recall_score(y_val, y_pred, average='weighted')

# Print results
print("Base logistic regression")
print(f"Validation Accuracy : {accuracy:.4f}")
print("Classification report on baseline LR model:")
print(classification_report(y_val, y_pred))
```


:::{.callout-warning title="Overfitting"}
-   We noticed earlier a very high correlation between certain features to the clearance status. This results in a model learning for example that whenever DiscountPercentage is high, predict ClearanceFlag=Yes.
-   A feature like `Revenue` is a *target-leaking* feature as it was engineered based on the ClearanceFlag status.
**Solution:** Drop any feature that is engineered from the target feature and observe the result.
:::

## Base model with "Revenue" (without "Revenue")

```{python}
#| code-fold: true
#| label: base-model-without-revenue

filtered_numerical_cols = ['OriginalPrice', 'QuantityAbs', 'Month', 'WeekOfYear', 'DayOfWeek']
selected_cat = ['StoreID', 'Brand', 'Supplier', 'Category', 'Subcategory', 'Type', 'Region', 'IsReturn', 'TransactionType']

# DictVectorizer encoding
dv = DictVectorizer(sparse=False)
train_dict = df_train[filtered_numerical_cols + selected_cat].to_dict(orient='records')
X_train = dv.fit_transform(train_dict)

# DictVectorizer encoding (val)
val_dict = df_val[filtered_numerical_cols + selected_cat].to_dict(orient='records')
X_val = dv.transform(val_dict)



# Train logistic regression
model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# Predict on validation
y_pred = model.predict(X_val)

# Evaluate performance
accuracy = accuracy_score(y_val, y_pred)


print("Logistic Regression (Leakage-Free Features)")
print(f"  Accuracy : {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_val, y_pred))
```

:::{.callout-note}
-   No overfitting, that is a good sign. But *underfitting*.
-   The base model never predicts class 1 (Clearance) as the accuracy is 80% which is also the percentage of observations that were sold at normal price.
-   Recall for class 1 = 0 indicating all actual clearance items are missed.
-   Precision for class 1 = 0 indicating when it predicts class 1 (which it doesn’t), it’s always wrong.
-   Macro avg: Treats both classes equally which exposes failure on class 1.
-   Weighted avg: Dominated by class 0 which looks better but hides the issue.

**Possible solutions:** 
    - Introduce feature engineering and observe the results. 
    - Try a sampling technique such oversampling or undersampling or SMOTE. - Use a tree-based models like decision tree.
:::

::::::


## IV.3. Feature Engineering

To enrich the features, we create new features to help the model learn better. The features will be created in the following groups:

-   Temporal features: 
    -   `IsWeekend`, `IsMonthEnd`, `IsMonthStart`, `IsPromotionSeason`
-   Price features:
    -   `DiscountCategory`, `DiscountedPriceCategory`, `PriceEfficiency`
-   Interaction features:
    -   `Region_Type`, `Region_Subcategory`, `Type_Subcategory`, `StoreID_Type`, `Supplier_Type`, etc
    -   `Supplier_return_rate`, `Brand_return_rate`
    
After feature engineering, we will attempt to make the model more robust by trying out the following:


-   Hyperparameter tuning
-   Smote
-   Oversampling
-   Undersampling